---
title: "Web Scaping"
output: html_document
---

```{r}
###for one page
library(rvest)
urls <- paste0("https://www.monster.com/jobs/search/Full-Time_8?q=data-scientist&kwdv=12258&page=", 1:40)

###looping, lapply
fields <- urls[1] %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "js_result_details-left", " " ))]')

job.urls <- sapply(fields, function(x) x %>% html_nodes("a") %>% html_attr("href"))
job.urls <- job.urls[1,]

titles <- fields %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "jobTitle", " " ))]//span') %>% html_text() %>% trimws()

names <- fields %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "company", " " ))]//span') %>% html_text() %>% trimws()

locations <- fields %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "job-specs-location", " " ))]//p') %>% html_text() %>% trimws()

#### Single URL example
description <- job.urls[1] %>% read_html() %>% html_nodes(xpath='//*[(@id = "JobBody")]//*[contains(concat( " ", @class, " " ), concat( " ", "card-content", " " ))]') %>% html_text()

grep("python", description, ignore.case=TRUE)
grep("\\bR\\b", description, ignore.case=TRUE)
grep("\\bSAS\\b", description, ignore.case=TRUE)


#### Find sector? These fields are not consistent across 
info <- job.urls[6] %>% read_html() %>% html_nodes(xpath='//*[(@id = "JobSummary")]//*[contains(concat( " ", @class, " " ), concat( " ", "card-content", " " ))]') %>% html_text()
info <- gsub("\r\n", ",", info)
if (grep("Industries", info) == 1){
  substr(info, regexpr("Industries", info) + 35, regexpr("Industries", info)+100)
}

### Just words
strsplit(trimws(description) ," ")

res <- sapply(job.urls, function(x) {
   desc <- x %>% read_html() %>% html_nodes(xpath='//*[(@id = "JobBody")]//*[contains(concat( " ", @class, " " ), concat( " ", "card-content", " " ))]') %>% html_text()
   python <- any(grepl("python", desc, ignore.case=TRUE))
   R <- any(grepl("\\bR\\b", desc, ignore.case=TRUE))
   SAS <- any(grepl("\\bSAS\\b", desc, ignore.case=TRUE))
   
   info <- x %>% read_html() %>% html_nodes(xpath='//*[(@id = "JobSummary")]//*[contains(concat( " ", @class, " " ), concat( " ", "card-content", " " ))]') %>% html_text()
   info <- gsub("\r\n", ",", info)
   if (length(grep("Industries", info)) != 0){
     ind <- substr(info, regexpr("Industries", info) + 35, regexpr("Industries", info)+100)
   }
   else{
     ind <- NA
   }
   
   c(python=python, R=R, SAS=SAS, Sector=ind)
 })
res <- unname(res)

data <- data.frame("Company" = names, "Location"=locations, "Sector"=res[4,],
                   "Python"=res[1,], "R"=res[2,], "SAS"=res[3,])


```


```{r}
###for 40 pages
library(rvest)
library(httr)
urls <- paste0("https://www.monster.com/jobs/search/Full-Time_8?q=Data-Scientist&page=", 1:40)

my_fun <- function(inx){
  fields <- inx %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "js_result_details-left", " " ))]')

  job.urls <- lapply(fields, function(x) x %>% html_nodes("a") %>% html_attr("href"))
  job.urls <- unlist(lapply(job.urls, function(x) x[[1]][1]))
  titles <- fields %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "jobTitle", " " ))]//span') %>% html_text() %>% trimws()
  names <- fields %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "company", " " ))]//span') %>% html_text() %>% trimws()
  if (length(names) == 27 & "Ciber" %in% names){
    names <- names[-c(which(names == "Ciber")+1, which(names == "Ciber")+2)]
  }
  else if(length(names) == 27 & "CGI" %in% names){
    names <- names[-c(which(names == "CGI")+1, which(names == "CGI")+2)]
  }
  else if(length(names) == 29){
    names <- names[-c(which(names == "Ciber")+1, which(names == "Ciber")+2)]
    names <- names[-c(which(names == "CGI")+1, which(names == "CGI")+2)]
  }
  locations <- fields %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "job-specs-location", " " ))]//p') %>% html_text() %>% trimws()
  
  
  res <- sapply(job.urls, function(x) {
    r <- GET(x, user_agent("myua"))
    if (status_code(r) >= 300){
      c(python=NA, R=NA, SAS=NA, Sector=NA)
    }
    else{
      desc <- x %>% read_html() %>% html_nodes(xpath='//*[(@id = "JobBody")]//*[contains(concat( " ", @class, " " ), concat( " ", "card-content", " " ))]') %>% html_text()
      python <- any(grepl("python", desc, ignore.case=TRUE))
      R <- any(grepl("\\bR\\b", desc, ignore.case=TRUE))
      SAS <- any(grepl("\\bSAS\\b", desc, ignore.case=TRUE))
   
      info <- x %>% read_html() %>% html_nodes(xpath='//*[(@id = "JobSummary")]//*[contains(concat( " ", @class, " " ), concat( " ", "card-content", " " ))]') %>% html_text()
      info <- gsub("\r\n", ",", info)
      if (length(grep("Industries", info)) != 0){
        ind <- substr(info, regexpr("Industries", info) + 35, regexpr("Industries", info)+100)
      }
      else{
        ind <- NA
      }
   
      c(python=python, R=R, SAS=SAS, Sector=ind)
    }
  })
  
  res <- unname(res)

  data.frame("Title" = titles,"Company" = names, "Location"=locations, "Sector"=res[4,],"Python"=res[1,], "R"=res[2,], "SAS"=res[3,])
  
}

data <- my_fun(urls[1])
for (i in 2:16){
  data <- rbind(data, my_fun(urls[i]))
}

##at i =17, 18, Error in data.frame(Title = titles, Company = names, Location = locations,  : 
##  arguments imply differing number of rows: 25, 27

for (i in 17:27){
  data <- rbind(data, my_fun(urls[i]))
}

##i = 28, Error in data.frame(Title = titles, Company = names, Location = locations,  : 
##arguments imply differing number of rows: 25, 0

for (i in 28:40){
  data <- rbind(data, my_fun(urls[i]))
}

```

